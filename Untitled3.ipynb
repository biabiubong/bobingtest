{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/biabiubong/bobingtest/blob/master/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lWmdYeV5L5ht"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import jieba\n",
        "\n",
        "jieba.setLogLevel('WARN')\n",
        "\n",
        "class data_transform():\n",
        "    def __init__(self):\n",
        "        self.data_path = None\n",
        "        self.data = None\n",
        "        self.texts_cut = None\n",
        "        self.tokenizer = None\n",
        "        self.label_set = {}\n",
        "        self.extraction = {}\n",
        "        self.tokenizer_fact = None\n",
        "\n",
        "    def read_data(self, path=None):\n",
        "        '''\n",
        "        读取json文件,必须readlines，否则中间有格式会报错\n",
        "        :param path: 文件路径\n",
        "        :return:json数据\n",
        "        eg. data_valid = data_transform.read_data(path='./data/data_valid.json')\n",
        "        '''\n",
        "        self.data_path = path\n",
        "        f = open(path, 'r', encoding='utf8')\n",
        "        data_raw = f.readlines()\n",
        "        data = []\n",
        "        for num, data_one in enumerate(data_raw):\n",
        "            try:\n",
        "                data.append(json.loads(data_one))\n",
        "            except Exception as e:\n",
        "                print('1')\n",
        "        self.data = data\n",
        "\n",
        "    def extract_data(self, name='accusation'):\n",
        "        '''\n",
        "        提取需要的信息，以字典形式存储\n",
        "        :param name: 提取内容\n",
        "        :return: 事实描述,罪名,相关法条\n",
        "        eg. data_valid_accusations = data_transform.extract_data(name='accusation')\n",
        "        '''\n",
        "        data = self.data\n",
        "        if name == 'fact':\n",
        "            extraction = list(map(lambda x: x['fact'], data))\n",
        "        elif name in ['accusation', 'relevant_articles']:\n",
        "            extraction = list(map(lambda x: x['meta'][name], data))\n",
        "        elif name == 'imprisonment':\n",
        "            extraction = []\n",
        "            for i in data:\n",
        "                if i['meta']['term_of_imprisonment']['death_penalty']:\n",
        "                    extraction.append([500])\n",
        "                elif i['meta']['term_of_imprisonment']['life_imprisonment']:\n",
        "                    extraction.append([400])\n",
        "                else:\n",
        "                    extraction.append([i['meta']['term_of_imprisonment']['imprisonment']])\n",
        "        self.extraction.update({name: extraction})\n",
        "\n",
        "    def cut_texts(self, texts=None, need_cut=True, word_len=1, texts_cut_savepath=None):\n",
        "        '''\n",
        "        文本分词剔除停用词\n",
        "        :param texts:文本列表\n",
        "        :param need_cut:是否需要分词\n",
        "        :param word_len:保留词语长度\n",
        "        :param texts_cut_savepath:保存路径\n",
        "        :return:\n",
        "        '''\n",
        "        if need_cut:\n",
        "            if word_len > 1:\n",
        "                texts_cut = [[word for word in jieba.lcut(one_text) if len(word) >= word_len] for one_text in texts]\n",
        "            else:\n",
        "                texts_cut = [jieba.lcut(one_text) for one_text in texts]\n",
        "        else:\n",
        "            if word_len > 1:\n",
        "                texts_cut = [[word for word in one_text if len(word) >= word_len] for one_text in texts]\n",
        "            else:\n",
        "                texts_cut = texts\n",
        "\n",
        "        if texts_cut_savepath is not None:\n",
        "            with open(texts_cut_savepath, 'w') as f:\n",
        "                json.dump(texts_cut, f)\n",
        "        return texts_cut\n",
        "\n",
        "    def text2seq(self, texts_cut=None, tokenizer_fact=None, num_words=2000, maxlen=30):\n",
        "        '''\n",
        "        文本转序列，训练集过大全部转换会内存溢出，每次放5000个样本\n",
        "        :param texts_cut: 分词后的文本列表\n",
        "        :param tokenizer:转换字典\n",
        "        :param num_words:字典词数量\n",
        "        :param maxlen:保留长度\n",
        "        :return:向量列表\n",
        "        eg. ata_transform.text2seq(texts_cut=train_fact_cut,num_words=2000, maxlen=500)\n",
        "        '''\n",
        "        texts_cut_len = len(texts_cut)\n",
        "\n",
        "        if tokenizer_fact is None:\n",
        "            tokenizer_fact = Tokenizer(num_words=num_words)\n",
        "            if texts_cut_len > 10000:\n",
        "                print('文本过多，分批转换')\n",
        "            n = 0\n",
        "            # 分批训练\n",
        "            while n < texts_cut_len:\n",
        "                tokenizer_fact.fit_on_texts(texts=texts_cut[n:n + 10000])\n",
        "                n += 10000\n",
        "                if n < texts_cut_len:\n",
        "                    print('tokenizer finish fit samples')\n",
        "                else:\n",
        "                    print('tokenizer finish fit samples')\n",
        "            self.tokenizer_fact = tokenizer_fact\n",
        "\n",
        "        # 全部转为数字序列\n",
        "        fact_seq = tokenizer_fact.texts_to_sequences(texts=texts_cut)\n",
        "        print('finish texts to sequences')\n",
        "\n",
        "        # 内存不够，删除\n",
        "        del texts_cut\n",
        "\n",
        "        n = 0\n",
        "        fact_pad_seq = []\n",
        "        # 分批执行pad_sequences\n",
        "        while n < texts_cut_len:\n",
        "            fact_pad_seq += list(pad_sequences(fact_seq[n:n + 10000], maxlen=maxlen,\n",
        "                                               padding='post', value=0, dtype='int'))\n",
        "            n += 10000\n",
        "            if n < texts_cut_len:\n",
        "                print('finish pad_sequences samples')\n",
        "            else:\n",
        "                print('finish pad_sequences samples')\n",
        "        self.fact_pad_seq = fact_pad_seq\n",
        "\n",
        "    def creat_label_set(self, name):\n",
        "        '''\n",
        "        获取标签集合，用于one-hot\n",
        "        :param name: 待创建集合的标签名称\n",
        "        :return:\n",
        "        '''\n",
        "        if name == 'accusation':\n",
        "            name_f = 'accu'\n",
        "        elif name == 'relevant_articles':\n",
        "            name_f = 'law'\n",
        "        with open('/content/accu.txt', encoding='utf-8') as f:\n",
        "            label_set = f.readlines()\n",
        "        label_set = [i[:-1] for i in label_set]\n",
        "        self.label_set.update({name: np.array(label_set)})\n",
        "\n",
        "    def creat_label(self, label, label_set):\n",
        "        '''\n",
        "        构建标签one-hot\n",
        "        :param label: 原始标签\n",
        "        :param label_set: 标签集合\n",
        "        :return: 标签one-hot\n",
        "        eg. creat_label(label=data_valid_accusations[12], label_set=accusations_set)\n",
        "        '''\n",
        "        label_str = [str(i) for i in label]\n",
        "        label_zero = np.zeros(len(label_set))\n",
        "        label_zero[np.in1d(label_set, label_str)] = 1\n",
        "        return label_zero\n",
        "\n",
        "    def creat_labels(self, label_set=None, labels=None, name='accusation'):\n",
        "        '''\n",
        "        调用creat_label遍历标签列表生成one-hot二维数组\n",
        "        :param label_set: 标签集合,数组\n",
        "        :param labels: 标签数据，二维列表，没有则调用extract_data函数提取\n",
        "        :param name:\n",
        "        :return:\n",
        "        '''\n",
        "        if label_set is None:\n",
        "            label_set = self.label_set[name]\n",
        "        if labels is None:\n",
        "            labels = self.extraction[name]\n",
        "        labels_one_hot = list(map(lambda x: self.creat_label(label=x, label_set=label_set), labels))\n",
        "        return labels_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umjUe8cXP6_h",
        "outputId": "df32bcc2-b20d-48de-c4b4-c05b60267395"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "finish big_fact_cut_0_100000\n",
            "finish big_fact_cut_100000_200000\n",
            "finish big_fact_cut_200000_300000\n",
            "finish big_fact_cut_300000_400000\n",
            "finish big_fact_cut_400000_500000\n",
            "finish big_fact_cut_500000_600000\n",
            "finish big_fact_cut_600000_700000\n",
            "finish big_fact_cut_700000_800000\n",
            "finish big_fact_cut_800000_900000\n",
            "finish big_fact_cut_900000_1000000\n",
            "finish big_fact_cut_1000000_1100000\n",
            "finish big_fact_cut_1100000_1200000\n",
            "finish big_fact_cut_1200000_1300000\n",
            "finish big_fact_cut_1300000_1400000\n",
            "finish big_fact_cut_1400000_1500000\n",
            "finish big_fact_cut_1500000_1600000\n",
            "finish big_fact_cut_1600000_1700000\n",
            "finish big_fact_cut_1700000_1800000\n",
            "start big_fact_cut_0_100000\n",
            "finish big_fact_cut_0_100000\n",
            "start big_fact_cut_100000_200000\n",
            "finish big_fact_cut_100000_200000\n",
            "start big_fact_cut_200000_300000\n",
            "finish big_fact_cut_200000_300000\n",
            "start big_fact_cut_300000_400000\n",
            "finish big_fact_cut_300000_400000\n",
            "start big_fact_cut_400000_500000\n",
            "finish big_fact_cut_400000_500000\n",
            "start big_fact_cut_500000_600000\n",
            "finish big_fact_cut_500000_600000\n",
            "start big_fact_cut_600000_700000\n",
            "finish big_fact_cut_600000_700000\n",
            "start big_fact_cut_700000_800000\n",
            "finish big_fact_cut_700000_800000\n",
            "start big_fact_cut_800000_900000\n",
            "finish big_fact_cut_800000_900000\n",
            "start big_fact_cut_900000_1000000\n",
            "finish big_fact_cut_900000_1000000\n",
            "start big_fact_cut_1000000_1100000\n",
            "finish big_fact_cut_1000000_1100000\n",
            "start big_fact_cut_1100000_1200000\n",
            "finish big_fact_cut_1100000_1200000\n",
            "start big_fact_cut_1200000_1300000\n",
            "finish big_fact_cut_1200000_1300000\n",
            "start big_fact_cut_1300000_1400000\n",
            "finish big_fact_cut_1300000_1400000\n",
            "start big_fact_cut_1400000_1500000\n",
            "finish big_fact_cut_1400000_1500000\n",
            "start big_fact_cut_1500000_1600000\n",
            "finish big_fact_cut_1500000_1600000\n",
            "start big_fact_cut_1600000_1700000\n",
            "finish big_fact_cut_1600000_1700000\n",
            "start big_fact_cut_1700000_1800000\n",
            "finish big_fact_cut_1700000_1800000\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pickle\n",
        "import jieba\n",
        "import numpy as np\n",
        "# -*- coding: utf-8 -*-\n",
        "jieba.setLogLevel('WARN')\n",
        "\n",
        "num_words = 40000\n",
        "maxlen = 400\n",
        "########################################################################################\n",
        "# big数据集处理\n",
        "data_transform_big = data_transform()\n",
        "\n",
        "# 读取json文件,1710857行\n",
        "data_transform_big.read_data(path='/content/data_test.json')\n",
        "\n",
        "# 提取需要信息\n",
        "data_transform_big.extract_data(name='fact')\n",
        "# big_fact = data_transform_big.extraction['fact']\n",
        "\n",
        "# 分词并保存原始分词结果，词语长度后期可以再改\n",
        "for i in range(18):\n",
        "    texts=data_transform_big.extraction['fact'][i*100000:(i*100000 + 100000)]\n",
        "    big_fact_cut = data_transform_big.cut_texts(texts=texts, word_len=1,\n",
        "                                                need_cut=True)\n",
        "    with open('/content/datadeal/data_cut/big_fact_cut_%d_%d.pkl' % (i*100000, i*100000 + 100000), mode='wb') as f:\n",
        "        pickle.dump(big_fact_cut, f)\n",
        "    print('finish big_fact_cut_%d_%d' % (i*100000, i*100000 + 100000))\n",
        "\n",
        "for i in range(18):\n",
        "    print('start big_fact_cut_%d_%d' % (i*100000, i*100000 + 100000))\n",
        "    with open('/content/datadeal/data_cut/big_fact_cut_%d_%d.pkl' % (i*100000, i*100000 + 100000), mode='rb') as f:\n",
        "        big_fact_cut = pickle.load(f)\n",
        "    data_transform_big = data_transform()\n",
        "    big_fact_cut_new = data_transform_big.cut_texts(texts=big_fact_cut,\n",
        "                                                    word_len=2,\n",
        "                                                    need_cut=False)\n",
        "    with open('/content/datadeal/data_cut/big_fact_cut_%d_%d_new.pkl' % (i*100000, i*100000 + 100000), mode='wb') as f:\n",
        "        pickle.dump(big_fact_cut_new, f)\n",
        "    print('finish big_fact_cut_%d_%d' % (i*100000, i*100000 + 100000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wFdjfqfnUUcM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b660953f-9c08-460d-e01b-b5ae3091f947"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "1\n",
            "1\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pickle\n",
        "import jieba\n",
        "import numpy as np\n",
        "# -*- coding: utf-8 -*-\n",
        "jieba.setLogLevel('WARN')\n",
        "\n",
        "num_words = 40000\n",
        "maxlen = 400\n",
        "########################################################################################\n",
        "# big数据集处理\n",
        "data_transform_big = data_transform()\n",
        "\n",
        "# 读取json文件,1710857行\n",
        "data_transform_big.read_data(path='/content/data_test.json')\n",
        "\n",
        "# 创建数据one-hot标签\n",
        "data_transform_big.extract_data(name='accusation')\n",
        "big_accusations = data_transform_big.extraction['accusation']\n",
        "data_transform_big.creat_label_set(name='accusation')\n",
        "big_labels = data_transform_big.creat_labels(name='accusation')\n",
        "np.save('/content/datadeal/labels/_accusation.npy', big_labels)\n",
        "\n",
        "# big数据集处理\n",
        "data_transform_big = data_transform()\n",
        "\n",
        "# 读取json文件,1710857行\n",
        "data_transform_big.read_data(path='/content/data_test.json')\n",
        "data_transform_big.extract_data(name='relevant_articles')\n",
        "big_relevant_articless = data_transform_big.extraction['relevant_articles']\n",
        "data_transform_big.creat_label_set(name='relevant_articles')\n",
        "big_labels = data_transform_big.creat_labels(name='relevant_articles')\n",
        "np.save('/content/datadeal/labels/big_labels_relevant_articles.npy', big_labels)\n",
        "\n",
        "# big数据集处理\n",
        "data_transform_big = data_transform()\n",
        "\n",
        "# 读取json文件,1710857行\n",
        "data_transform_big.read_data(path='/content/data_test.json')\n",
        "\n",
        "# 创建刑期连续变量\n",
        "data_transform_big.extract_data(name='imprisonment')\n",
        "big_imprisonments = data_transform_big.extraction['imprisonment']\n",
        "np.save('/content/datadeal/labels/big_labels_imprisonments.npy', big_imprisonments)\n",
        "\n",
        "# big数据集处理\n",
        "data_transform_big = data_transform()\n",
        "\n",
        "# 读取json文件,1710857行\n",
        "data_transform_big.read_data(path='/content/data_test.json')\n",
        "\n",
        "# 创建刑期离散变量\n",
        "data_transform_big.extract_data(name='imprisonment')\n",
        "big_imprisonments = data_transform_big.extraction['imprisonment']\n",
        "data_transform_big.creat_label_set(name='imprisonment')\n",
        "big_labels = data_transform_big.creat_labels(name='imprisonment')\n",
        "np.save('/content/datadeal/labels/big_labels_imprisonments_discrete.npy', big_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgX21UpoYXV_",
        "outputId": "25241012-b5a3-4f6e-aea4-6680dedf89fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start big_fact_cut_0_100000\n",
            "tokenizer finish fit 6344 samples\n",
            "finish big_fact_cut_0_100000\n",
            "start big_fact_cut_100000_200000\n",
            "finish big_fact_cut_100000_200000\n",
            "start big_fact_cut_200000_300000\n",
            "finish big_fact_cut_200000_300000\n",
            "start big_fact_cut_300000_400000\n",
            "finish big_fact_cut_300000_400000\n",
            "start big_fact_cut_400000_500000\n",
            "finish big_fact_cut_400000_500000\n",
            "start big_fact_cut_500000_600000\n",
            "finish big_fact_cut_500000_600000\n",
            "start big_fact_cut_600000_700000\n",
            "finish big_fact_cut_600000_700000\n",
            "start big_fact_cut_700000_800000\n",
            "finish big_fact_cut_700000_800000\n",
            "start big_fact_cut_800000_900000\n",
            "finish big_fact_cut_800000_900000\n",
            "start big_fact_cut_900000_1000000\n",
            "finish big_fact_cut_900000_1000000\n",
            "start big_fact_cut_1000000_1100000\n",
            "finish big_fact_cut_1000000_1100000\n",
            "start big_fact_cut_1100000_1200000\n",
            "finish big_fact_cut_1100000_1200000\n",
            "start big_fact_cut_1200000_1300000\n",
            "finish big_fact_cut_1200000_1300000\n",
            "start big_fact_cut_1300000_1400000\n",
            "finish big_fact_cut_1300000_1400000\n",
            "start big_fact_cut_1400000_1500000\n",
            "finish big_fact_cut_1400000_1500000\n",
            "start big_fact_cut_1500000_1600000\n",
            "finish big_fact_cut_1500000_1600000\n",
            "start big_fact_cut_1600000_1700000\n",
            "finish big_fact_cut_1600000_1700000\n",
            "start big_fact_cut_1700000_1800000\n",
            "finish big_fact_cut_1700000_1800000\n",
            "start big_fact_cut_0_100000\n",
            "finish big_fact_cut_0_100000\n",
            "start big_fact_cut_100000_200000\n",
            "finish big_fact_cut_100000_200000\n",
            "start big_fact_cut_200000_300000\n",
            "finish big_fact_cut_200000_300000\n",
            "start big_fact_cut_300000_400000\n",
            "finish big_fact_cut_300000_400000\n",
            "start big_fact_cut_400000_500000\n",
            "finish big_fact_cut_400000_500000\n",
            "start big_fact_cut_500000_600000\n",
            "finish big_fact_cut_500000_600000\n",
            "start big_fact_cut_600000_700000\n",
            "finish big_fact_cut_600000_700000\n",
            "start big_fact_cut_700000_800000\n",
            "finish big_fact_cut_700000_800000\n",
            "start big_fact_cut_800000_900000\n",
            "finish big_fact_cut_800000_900000\n",
            "start big_fact_cut_900000_1000000\n",
            "finish big_fact_cut_900000_1000000\n",
            "start big_fact_cut_1000000_1100000\n",
            "finish big_fact_cut_1000000_1100000\n",
            "start big_fact_cut_1100000_1200000\n",
            "finish big_fact_cut_1100000_1200000\n",
            "start big_fact_cut_1200000_1300000\n",
            "finish big_fact_cut_1200000_1300000\n",
            "start big_fact_cut_1300000_1400000\n",
            "finish big_fact_cut_1300000_1400000\n",
            "start big_fact_cut_1400000_1500000\n",
            "finish big_fact_cut_1400000_1500000\n",
            "start big_fact_cut_1500000_1600000\n",
            "finish big_fact_cut_1500000_1600000\n",
            "start big_fact_cut_1600000_1700000\n",
            "finish big_fact_cut_1600000_1700000\n",
            "start big_fact_cut_1700000_1800000\n",
            "finish big_fact_cut_1700000_1800000\n",
            "start big_fact_cut_0_100000\n",
            "finish pad_sequences 6344 samples\n",
            "start big_fact_cut_100000_200000\n",
            "start big_fact_cut_200000_300000\n",
            "start big_fact_cut_300000_400000\n",
            "start big_fact_cut_400000_500000\n",
            "start big_fact_cut_500000_600000\n",
            "start big_fact_cut_600000_700000\n",
            "start big_fact_cut_700000_800000\n",
            "start big_fact_cut_800000_900000\n",
            "start big_fact_cut_900000_1000000\n",
            "start big_fact_cut_1000000_1100000\n",
            "start big_fact_cut_1100000_1200000\n",
            "start big_fact_cut_1200000_1300000\n",
            "start big_fact_cut_1300000_1400000\n",
            "start big_fact_cut_1400000_1500000\n",
            "start big_fact_cut_1500000_1600000\n",
            "start big_fact_cut_1600000_1700000\n",
            "start big_fact_cut_1700000_1800000\n",
            "start big_fact_cut_0_100000\n",
            "start big_fact_cut_100000_200000\n",
            "start big_fact_cut_200000_300000\n",
            "start big_fact_cut_300000_400000\n",
            "start big_fact_cut_400000_500000\n",
            "start big_fact_cut_500000_600000\n",
            "start big_fact_cut_600000_700000\n",
            "start big_fact_cut_700000_800000\n",
            "start big_fact_cut_800000_900000\n",
            "start big_fact_cut_900000_1000000\n",
            "start big_fact_cut_1000000_1100000\n",
            "start big_fact_cut_1100000_1200000\n",
            "start big_fact_cut_1200000_1300000\n",
            "start big_fact_cut_1300000_1400000\n",
            "start big_fact_cut_1400000_1500000\n",
            "start big_fact_cut_1500000_1600000\n",
            "start big_fact_cut_1600000_1700000\n",
            "start big_fact_cut_1700000_1800000\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import jieba\n",
        "import json\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "jieba.setLogLevel('WARN')\n",
        "\n",
        "num_words = 80000\n",
        "maxlen = 400\n",
        "\n",
        "tokenizer_fact = Tokenizer(num_words=num_words)\n",
        "\n",
        "for i in range(18):\n",
        "    print('start big_fact_cut_%d_%d' % (i * 100000, i * 100000 + 100000))\n",
        "    with open('/content/datadeal/data_cut/big_fact_cut_%d_%d_new.pkl' % (i * 100000, i * 100000 + 100000), mode='rb') as f:\n",
        "        big_fact_cut = pickle.load(f)\n",
        "    texts_cut_len = len(big_fact_cut)\n",
        "    n = 0\n",
        "    # 分批训练\n",
        "    while n < texts_cut_len:\n",
        "        tokenizer_fact.fit_on_texts(texts=big_fact_cut[n:n + 10000])\n",
        "        n += 10000\n",
        "        if n < texts_cut_len:\n",
        "            print('tokenizer finish fit %d samples' % n)\n",
        "        else:\n",
        "            print('tokenizer finish fit %d samples' % texts_cut_len)\n",
        "    print('finish big_fact_cut_%d_%d' % (i * 100000, i * 100000 + 100000))\n",
        "\n",
        "with open('/content/model/tokenizer_fact_%d.pkl' % (num_words), mode='wb') as f:\n",
        "    pickle.dump(tokenizer_fact, f)\n",
        "\n",
        "with open('/content/model/tokenizer_fact_%d.pkl' % (num_words), mode='rb') as f:\n",
        "    tokenizer_fact=pickle.load(f)\n",
        "\n",
        "# texts_to_sequences\n",
        "for i in range(18):\n",
        "    print('start big_fact_cut_%d_%d' % (i * 100000, i * 100000 + 100000))\n",
        "    with open('/content/datadeal/data_cut/big_fact_cut_%d_%d_new.pkl' % (i * 100000, i * 100000 + 100000), mode='rb') as f:\n",
        "        big_fact_cut = pickle.load(f)\n",
        "    # 分批执行 texts_to_sequences\n",
        "    big_fact_seq = tokenizer_fact.texts_to_sequences(texts=big_fact_cut)\n",
        "    with open('/content/datadeal/fact_seq/fact_seq_%d_%d.pkl' % (i * 100000, i * 100000 + 100000), mode='wb') as f:\n",
        "        pickle.dump(big_fact_seq, f)\n",
        "    print('finish big_fact_cut_%d_%d' % (i * 100000, i * 100000 + 100000))\n",
        "\n",
        "# pad_sequences\n",
        "for i in range(18):\n",
        "    print('start big_fact_cut_%d_%d' % (i * 100000, i * 100000 + 100000))\n",
        "    with open('/content/datadeal/fact_seq/fact_seq_%d_%d.pkl' % (i * 100000, i * 100000 + 100000), mode='rb') as f:\n",
        "        big_fact_seq = pickle.load(f)\n",
        "    texts_cut_len = len(big_fact_seq)\n",
        "    n = 0\n",
        "    fact_pad_seq = []\n",
        "    # 分批执行pad_sequences\n",
        "    while n < texts_cut_len:\n",
        "        fact_pad_seq += list(pad_sequences(big_fact_seq[n:n + 20000], maxlen=maxlen,\n",
        "                                           padding='post', value=0, dtype='int'))\n",
        "        n += 20000\n",
        "        if n < texts_cut_len:\n",
        "            print('finish pad_sequences %d samples' % n)\n",
        "        else:\n",
        "            print('finish pad_sequences %d samples' % texts_cut_len)\n",
        "    with open('/content/datadeal/fact_pad_seq/fact_pad_seq_%d_%d_%d.pkl' % (maxlen, i * 100000, i * 100000 + 100000),\n",
        "              mode='wb') as f:\n",
        "        pickle.dump(fact_pad_seq, f)\n",
        "\n",
        "# 汇总pad_sequences,5G,16G内存够用\n",
        "maxlen = 400\n",
        "num_words = 40000\n",
        "fact_pad_seq = []\n",
        "for i in range(18):\n",
        "    print('start big_fact_cut_%d_%d' % (i * 100000, i * 100000 + 100000))\n",
        "    with open('/content/datadeal/fact_pad_seq/fact_pad_seq_%d_%d_%d.pkl' % (maxlen, i * 100000, i * 100000 + 100000),\n",
        "              mode='rb') as f:\n",
        "        fact_pad_seq += pickle.load(f)\n",
        "fact_pad_seq = np.array(fact_pad_seq)\n",
        "np.save('/content/datadeal/fact_pad_seq/big_fact_pad_seq_%d_%d.npy' % (num_words, maxlen), fact_pad_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kits6zPDbmZU",
        "outputId": "230a2c3c-c1fc-4ec8-b2b0-5f0bd2095809"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8166666666666667\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "def predict2half(predictions):\n",
        "    return np.where(predictions > 0.5, 1.0, 0.0)\n",
        "\n",
        "\n",
        "def predict2top(predictions):\n",
        "    one_hots = []\n",
        "    for prediction in predictions:\n",
        "        one_hot = np.where(prediction == prediction.max(), 1.0, 0.0)\n",
        "        one_hots.append(one_hot)\n",
        "    return np.array(one_hots)\n",
        "\n",
        "\n",
        "def predict2both(predictions):\n",
        "    one_hots = []\n",
        "    for prediction in predictions:\n",
        "        one_hot = np.where(prediction > 0.5, 1.0, 0.0)\n",
        "        if one_hot.sum() == 0:\n",
        "            one_hot = np.where(prediction == prediction.max(), 1.0, 0.0)\n",
        "        one_hots.append(one_hot)\n",
        "    return np.array(one_hots)\n",
        "\n",
        "\n",
        "def f1_avg(y_pred, y_true):\n",
        "    '''\n",
        "    mission 1&2\n",
        "    :param y_pred:\n",
        "    :param y_true:\n",
        "    :return:\n",
        "    '''\n",
        "    f1_micro = f1_score(y_pred=y_pred, y_true=y_true, pos_label=1, average='micro')\n",
        "    f1_macro = f1_score(y_pred=y_pred, y_true=y_true, pos_label=1, average='macro')\n",
        "    return (f1_micro + f1_macro) / 2\n",
        "\n",
        "\n",
        "def distance_score(y_true, y_pred):\n",
        "    '''\n",
        "    mission 3\n",
        "    :param y_true:\n",
        "    :param y_pred:\n",
        "    :return:\n",
        "    '''\n",
        "    result = 0\n",
        "    n = len(y_true)\n",
        "    for i in range(n):\n",
        "        v = np.abs(np.log10(y_true[i][0] + 1) - np.log10(y_pred[i][0] + 1))\n",
        "        if y_true[i][0] == 500:\n",
        "            if y_pred[i][0] > 400:\n",
        "                result += 1 / n\n",
        "        elif y_true[i][0] == 400:\n",
        "            if y_pred[i][0] <= 400 and y_pred[i][0] > 300:\n",
        "                result += 1 / n\n",
        "        else:\n",
        "            if v <= 0.2:\n",
        "                result += 1 / n\n",
        "            elif v <= 0.4:\n",
        "                result += 0.8 / n\n",
        "            elif v <= 0.6:\n",
        "                result += 0.6 / n\n",
        "            elif v <= 0.8:\n",
        "                result += 0.4 / n\n",
        "            elif v <= 1.0:\n",
        "                result += 0.2 / n\n",
        "            else:\n",
        "                pass\n",
        "    return result\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(f1_avg(y_pred=np.array([[0, 1], [1, 0]]),\n",
        "                 y_true=np.array([[0, 1], [1, 1]])))\n",
        "\n",
        "from keras.layers import Conv1D, BatchNormalization, Activation, GlobalMaxPool1D\n",
        "\n",
        "\n",
        "def textcnn_one(word_vec=None, kernel_size=1, filters=512):\n",
        "    x = word_vec\n",
        "    x = Conv1D(filters=filters, kernel_size=[kernel_size], strides=1, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(activation='relu')(x)\n",
        "    x = Conv1D(filters=filters, kernel_size=[kernel_size], strides=1, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(activation='relu')(x)\n",
        "    x = GlobalMaxPool1D()(x)\n",
        "\n",
        "    return x\n",
        "if __name__ == '__main__':\n",
        "    from keras.layers import Dense, Embedding, Input, Dropout\n",
        "    from keras.layers import BatchNormalization, Concatenate\n",
        "    from keras.models import Model\n",
        "    from keras.utils import plot_model\n",
        "\n",
        "    filters=256\n",
        "    data_input = Input(shape=[400])\n",
        "    word_vec = Embedding(input_dim=40000 + 1,\n",
        "                         input_length=400,\n",
        "                         output_dim=512,\n",
        "                         mask_zero=False,\n",
        "                         name='Embedding')(data_input)\n",
        "\n",
        "    x1 = textcnn_one(word_vec=word_vec, kernel_size=1, filters=filters)\n",
        "    x2 = textcnn_one(word_vec=word_vec, kernel_size=2, filters=filters)\n",
        "    x3 = textcnn_one(word_vec=word_vec, kernel_size=3, filters=filters)\n",
        "    x4 = textcnn_one(word_vec=word_vec, kernel_size=4, filters=filters)\n",
        "    x5 = textcnn_one(word_vec=word_vec, kernel_size=5, filters=filters)\n",
        "\n",
        "    x = Concatenate(axis=1)([x1, x2, x3, x4, x5])\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(500, activation=\"relu\")(x)\n",
        "    x = Dense(202, activation=\"sigmoid\")(x)\n",
        "    model = Model(inputs=data_input, outputs=x)\n",
        "    plot_model(model, './textcnn.png', show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwgAhheUZw_2",
        "outputId": "acbb4972-ec22-4359-e66a-844d0d2d5936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start 2024-04-21 08:03:59\n",
            "accusation\n",
            "num_words = 40000, maxlen = 400\n",
            " 1/12 [=>............................] - ETA: 18:41 - loss: 0.8402 - accuracy: 0.0059"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Embedding, Input,Dropout\n",
        "from keras.layers import BatchNormalization, Concatenate\n",
        "import pandas as pd\n",
        "import time\n",
        "from keras.models import load_model\n",
        "#训练text_cnn\n",
        "\n",
        "print('start', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
        "print('accusation')\n",
        "data_transform_big = data_transform()\n",
        "num_words = 80000\n",
        "maxlen = 400\n",
        "filters = 256\n",
        "print('num_words = 40000, maxlen = 400')\n",
        "\n",
        "# fact数据集\n",
        "fact = np.load('/content/datadeal/fact_pad_seq/big_fact_pad_seq_40000_400.npy')\n",
        "fact_train, fact_test = train_test_split(fact, test_size=0.05, random_state=1)\n",
        "del fact\n",
        "\n",
        "# 标签数据集\n",
        "labels = np.load('/content/datadeal/labels/_accusation.npy')\n",
        "labels_train, labels_test = train_test_split(labels, test_size=0.05, random_state=1)\n",
        "del labels\n",
        "set_accusation = data_transform_big.creat_label_set(name='accusation')\n",
        "# set_accusation = np.load('/content/datadeal/set/set_accusation.npy')\n",
        "\n",
        "data_input = Input(shape=[maxlen])\n",
        "word_vec = Embedding(input_dim=num_words + 1,\n",
        "                     input_length=maxlen,\n",
        "                     output_dim=512,\n",
        "                     mask_zero=False,\n",
        "                     name='Embedding')(data_input)\n",
        "\n",
        "x1 = textcnn_one(word_vec=word_vec, kernel_size=1, filters=filters)\n",
        "x2 = textcnn_one(word_vec=word_vec, kernel_size=2, filters=filters)\n",
        "x3 = textcnn_one(word_vec=word_vec, kernel_size=3, filters=filters)\n",
        "x4 = textcnn_one(word_vec=word_vec, kernel_size=4, filters=filters)\n",
        "x5 = textcnn_one(word_vec=word_vec, kernel_size=5, filters=filters)\n",
        "\n",
        "x = Concatenate(axis=1)([x1, x2, x3, x4, x5])\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(1000, activation=\"relu\")(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(labels_train.shape[1], activation=\"sigmoid\")(x)\n",
        "model = Model(inputs=data_input, outputs=x)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "n_start = 1\n",
        "n_end = 21\n",
        "score_list1 = []\n",
        "score_list2 = []\n",
        "for i in range(n_start, n_end):\n",
        "    model.fit(x=fact_train, y=labels_train, batch_size=512, epochs=1, verbose=1)\n",
        "\n",
        "    model.save('/content/model/%d_%d/accusation/TextCNN_%d_epochs_%d.h5' % (num_words, maxlen, filters, i))\n",
        "\n",
        "    y = model.predict(fact_test[:])\n",
        "    y1 = predict2top(y)\n",
        "    y2 = predict2half(y)\n",
        "    y3 = predict2both(y)\n",
        "\n",
        "    print('%s accu:' % i)\n",
        "    # 只取最高置信度的准确率\n",
        "    s1 = [(labels_test[i] == y1[i]).min() for i in range(len(y1))]\n",
        "    print(sum(s1) / len(s1))\n",
        "    # 只取置信度大于0.5的准确率\n",
        "    s2 = [(labels_test[i] == y2[i]).min() for i in range(len(y1))]\n",
        "    print(sum(s2) / len(s2))\n",
        "    # 结合前两个\n",
        "    s3 = [(labels_test[i] == y3[i]).min() for i in range(len(y1))]\n",
        "    print(sum(s3) / len(s3))\n",
        "\n",
        "    print('%s f1:' % i)\n",
        "    # 只取最高置信度的准确率\n",
        "    s4 = f1_avg(y_pred=y1, y_true=labels_test)\n",
        "    print(s4)\n",
        "    # 只取置信度大于0.5的准确率\n",
        "    s5 = f1_avg(y_pred=y2, y_true=labels_test)\n",
        "    print(s5)\n",
        "    # 结合前两个\n",
        "    s6 = f1_avg(y_pred=y3, y_true=labels_test)\n",
        "    print(s6)\n",
        "\n",
        "    score_list1.append([i,\n",
        "                        sum(s1) / len(s1),\n",
        "                        sum(s2) / len(s2),\n",
        "                        sum(s3) / len(s3)])\n",
        "    score_list2.append([i, s4, s5, s6])\n",
        "\n",
        "print(pd.DataFrame(score_list1))\n",
        "print(pd.DataFrame(score_list2))\n",
        "print('end', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
        "print('#####################\\n')\n",
        "# nohup python model_CNN_accusation.py 2>&1 &"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Conv1D, GlobalMaxPool1D\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.layers import GRU, MaxPooling1D, Bidirectional\n",
        "import pandas as pd\n",
        "import time\n",
        "from keras.models import load_model\n",
        "#训练纯cnn\n",
        "\n",
        "\n",
        "print('start', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
        "print('accusation')\n",
        "\n",
        "num_words = 80000\n",
        "maxlen = 400\n",
        "kernel_size = 3\n",
        "DIM = 512\n",
        "batch_size = 256\n",
        "\n",
        "print('num_words = 40000, maxlen = 400 ')\n",
        "\n",
        "# fact数据集\n",
        "fact = np.load('/content/datadeal/fact_pad_seq/big_fact_pad_seq_40000_400.npy')\n",
        "fact_train, fact_test = train_test_split(fact, test_size=0.05, random_state=1)\n",
        "del fact\n",
        "\n",
        "# 标签数据集\n",
        "labels = np.load('/content/datadeal/labels/_accusation.npy')\n",
        "labels_train, labels_test = train_test_split(labels, test_size=0.05, random_state=1)\n",
        "del labels\n",
        "\n",
        "# 数据增强\n",
        "maxcount = 10000\n",
        "num = 100\n",
        "# index_add_accusation = np.load('')\n",
        "# fact_train = np.concatenate([fact_train, fact_train[index_add_accusation]], axis=0)\n",
        "# labels_train = np.concatenate([labels_train, labels_train[index_add_accusation]], axis=0)\n",
        "\n",
        "data_input = Input(shape=[fact_train.shape[1]])\n",
        "word_vec = Embedding(input_dim=num_words + 1,\n",
        "                     input_length=maxlen,\n",
        "                     output_dim=DIM,\n",
        "                     mask_zero=0,\n",
        "                     name='Embedding')(data_input)\n",
        "x = word_vec\n",
        "x = Conv1D(filters=512, kernel_size=[kernel_size], strides=1, padding='same', activation='relu')(x)\n",
        "x = GlobalMaxPool1D()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(1000, activation=\"relu\")(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(labels_train.shape[1], activation=\"sigmoid\")(x)\n",
        "model = Model(inputs=data_input, outputs=x)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "# model.summary()\n",
        "n_start = 1\n",
        "n_end = 21\n",
        "score_list1 = []\n",
        "score_list2 = []\n",
        "\n",
        "for i in range(n_start, n_end):\n",
        "    model.fit(x=fact_train, y=labels_train, batch_size=batch_size, epochs=1, verbose=1)\n",
        "\n",
        "    model.save('./model/%d_%d/accusation/CNN_epochs_%d.h5' % (num_words, maxlen, i))\n",
        "\n",
        "    y = model.predict(fact_test[:])\n",
        "    y1 = predict2top(y)\n",
        "    y2 = predict2half(y)\n",
        "    y3 = predict2both(y)\n",
        "\n",
        "    print('%s accu:' % i)\n",
        "    # 只取最高置信度的准确率\n",
        "    s1 = [(labels_test[i] == y1[i]).min() for i in range(len(y1))]\n",
        "    print(sum(s1) / len(s1))\n",
        "    # 只取置信度大于0.5的准确率\n",
        "    s2 = [(labels_test[i] == y2[i]).min() for i in range(len(y1))]\n",
        "    print(sum(s2) / len(s2))\n",
        "    # 结合前两个\n",
        "    s3 = [(labels_test[i] == y3[i]).min() for i in range(len(y1))]\n",
        "    print(sum(s3) / len(s3))\n",
        "\n",
        "    print('%s f1:' % i)\n",
        "    # 只取最高置信度的准确率\n",
        "    s4 = f1_avg(y_pred=y1, y_true=labels_test)\n",
        "    print(s4)\n",
        "    # 只取置信度大于0.5的准确率\n",
        "    s5 = f1_avg(y_pred=y2, y_true=labels_test)\n",
        "    print(s5)\n",
        "    # 结合前两个\n",
        "    s6 = f1_avg(y_pred=y3, y_true=labels_test)\n",
        "    print(s6)\n",
        "\n",
        "    score_list1.append([i,\n",
        "                        sum(s1) / len(s1),\n",
        "                        sum(s2) / len(s2),\n",
        "                        sum(s3) / len(s3)])\n",
        "    score_list2.append([i, s4, s5, s6])\n",
        "    print(pd.DataFrame(score_list1))\n",
        "    print(pd.DataFrame(score_list2))\n",
        "\n",
        "print('end', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
        "print('#####################\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_TIQXDmmka-",
        "outputId": "d036b672-783d-44e1-ce0c-17117d4f3883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start 2024-04-21 08:07:05\n",
            "accusation\n",
            "num_words = 40000, maxlen = 400 \n",
            " 3/24 [==>...........................] - ETA: 4:51 - loss: 0.6306 - accuracy: 0.0195"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras.utils import plot_model\n",
        "\n",
        "\n",
        "def attention(input=None, depth=None):\n",
        "    attention = Dense(1, activation='tanh')(input)\n",
        "    attention = Flatten()(attention)\n",
        "    attention = Activation('softmax')(attention)\n",
        "    attention = RepeatVector(depth)(attention)\n",
        "    attention = Permute([2, 1], name='attention_vec')(attention)\n",
        "    attention_mul = Multiply(name='attention_mul')([input, attention])\n",
        "    return attention_mul\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    data_input = Input(shape=[400])\n",
        "    word_vec = Embedding(input_dim=40000 + 1,\n",
        "                         input_length=400,\n",
        "                         output_dim=512,\n",
        "                         mask_zero=False,\n",
        "                         name='Embedding')(data_input)\n",
        "    x = word_vec\n",
        "    x = Conv1D(filters=512, kernel_size=[3], strides=1, padding='same', activation='relu')(x)\n",
        "    x = attention(input=x, depth=512)\n",
        "    x = GlobalMaxPool1D()(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(500, activation=\"relu\")(x)\n",
        "    x = Dense(202, activation=\"sigmoid\")(x)\n",
        "    model = Model(inputs=data_input, outputs=x)\n",
        "    plot_model(model, './attention.png', show_shapes=True)"
      ],
      "metadata": {
        "id": "SiJr_51UtS-G"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cnn_attention训练\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Conv1D, GlobalMaxPool1D\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.layers import GRU, MaxPooling1D, Bidirectional\n",
        "import pandas as pd\n",
        "import time\n",
        "from keras.models import load_model\n",
        "\n",
        "data_transform_big = data_transform()\n",
        "print('start', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
        "print('accusation')\n",
        "set_accusation = data_transform_big.creat_label_set(name='accusation')\n",
        "num_words = 80000\n",
        "maxlen = 400\n",
        "\n",
        "print('num_words = 40000, maxlen = 400')\n",
        "\n",
        "# fact数据集\n",
        "fact = np.load('/content/datadeal/fact_pad_seq/big_fact_pad_seq_40000_400.npy')\n",
        "fact_train, fact_test = train_test_split(fact, test_size=0.05, random_state=1)\n",
        "del fact\n",
        "\n",
        "# 标签数据集\n",
        "labels = np.load('/content/datadeal/labels/_accusation.npy')\n",
        "labels_train, labels_test = train_test_split(labels, test_size=0.05, random_state=1)\n",
        "del labels\n",
        "\n",
        "\n",
        "\n",
        "data_input = Input(shape=[maxlen])\n",
        "word_vec = Embedding(input_dim=num_words + 1,\n",
        "                     input_length=maxlen,\n",
        "                     output_dim=512,\n",
        "                     mask_zero=0,\n",
        "                     name='Embedding')(data_input)\n",
        "x = word_vec\n",
        "x = Conv1D(filters=512, kernel_size=[3], strides=1, padding='same', activation='relu')(x)\n",
        "x = attention(input=x, depth=512)\n",
        "x = GlobalMaxPool1D()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(1000, activation=\"relu\")(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(labels_train.shape[1], activation=\"sigmoid\")(x)\n",
        "model = Model(inputs=data_input, outputs=x)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "n_start = 1\n",
        "n_end = 21\n",
        "score_list1 = []\n",
        "score_list2 = []\n",
        "for i in range(n_start, n_end):\n",
        "    model.fit(x=fact_train, y=labels_train, batch_size=512, epochs=1, verbose=1)\n",
        "\n",
        "    model.save('/content/model/CNN_attention_epochs_%d.h5' % (num_words, maxlen, i))\n",
        "\n",
        "    y = model.predict(fact_test[:])\n",
        "    y1 = predict2top(y)\n",
        "    y2 = predict2half(y)\n",
        "    y3 = predict2both(y)\n",
        "\n",
        "    print('%s accu:' % i)\n",
        "    # 只取最高置信度的准确率\n",
        "    s1 = [(labels_test[i] == y1[i]).min() for i in range(len(y1))]\n",
        "    print(sum(s1) / len(s1))\n",
        "    # 只取置信度大于0.5的准确率\n",
        "    s2 = [(labels_test[i] == y2[i]).min() for i in range(len(y1))]\n",
        "    print(sum(s2) / len(s2))\n",
        "    # 结合前两个\n",
        "    s3 = [(labels_test[i] == y3[i]).min() for i in range(len(y1))]\n",
        "    print(sum(s3) / len(s3))\n",
        "\n",
        "    print('%s f1:' % i)\n",
        "    # 只取最高置信度的准确率\n",
        "    s4 = f1_avg(y_pred=y1, y_true=labels_test)\n",
        "    print(s4)\n",
        "    # 只取置信度大于0.5的准确率\n",
        "    s5 = f1_avg(y_pred=y2, y_true=labels_test)\n",
        "    print(s5)\n",
        "    # 结合前两个\n",
        "    s6 = f1_avg(y_pred=y3, y_true=labels_test)\n",
        "    print(s6)\n",
        "\n",
        "    score_list1.append([i,\n",
        "                        sum(s1) / len(s1),\n",
        "                        sum(s2) / len(s2),\n",
        "                        sum(s3) / len(s3)])\n",
        "    score_list2.append([i, s4, s5, s6])\n",
        "    print(pd.DataFrame(score_list1))\n",
        "    print(pd.DataFrame(score_list2))\n",
        "\n",
        "print('end', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "PcQOVpjisqvw",
        "outputId": "0f36ba7e-45d4-4090-b123-56b2559bc0dc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start 2024-04-21 08:12:39\n",
            "accusation\n",
            "num_words = 40000, maxlen = 400\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ed63955980bd>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mscore_list2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfact_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/model/CNN_attention_epochs_%d.h5'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;31m# no_variable_creation function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m         return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    906\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, Embedding\n",
        "from keras.layers import GlobalMaxPool1D, Dropout, Conv1D, BatchNormalization, Activation, Add\n",
        "from keras.utils import plot_model\n",
        "\n",
        "\n",
        "def block(x, kernel_size):\n",
        "    x_Conv_1 = Conv1D(filters=512, kernel_size=[kernel_size], strides=1, padding='same')(x)\n",
        "    x_Conv_1 = Activation(activation='relu')(x_Conv_1)\n",
        "    x_Conv_2 = Conv1D(filters=512, kernel_size=[kernel_size], strides=1, padding='same')(x_Conv_1)\n",
        "    x_Conv_2 = Add()([x, x_Conv_2])\n",
        "    x = Activation(activation='relu')(x_Conv_2)\n",
        "    return x\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    num_words = 80000\n",
        "    maxlen = 400\n",
        "    kernel_size = 3\n",
        "    DIM = 512\n",
        "    batch_size = 256\n",
        "\n",
        "    data_input = Input(shape=[maxlen])\n",
        "    word_vec = Embedding(input_dim=num_words + 1,\n",
        "                         input_length=maxlen,\n",
        "                         output_dim=DIM,\n",
        "                         mask_zero=0,\n",
        "                         name='Embedding')(data_input)\n",
        "    block1 = block(x=word_vec, kernel_size=3)\n",
        "    block2 = block(x=block1, kernel_size=3)\n",
        "    x = GlobalMaxPool1D()(block2)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(1000, activation=\"relu\")(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(202, activation=\"sigmoid\")(x)\n",
        "    model = Model(inputs=data_input, outputs=x)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    plot_model(model, './resnet.png', show_shapes=True)"
      ],
      "metadata": {
        "id": "N-rtBoYIuaye"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Conv1D, GlobalMaxPool1D\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.layers import GRU, MaxPooling1D, Bidirectional\n",
        "import pandas as pd\n",
        "import time\n",
        "# from resnet import block\n",
        "# from evaluate import predict2both, predict2half, predict2top, f1_avg\n",
        "from keras.models import load_model\n",
        "#resmodel训练\n",
        "print('start', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
        "print('accusation')\n",
        "\n",
        "num_words = 80000\n",
        "maxlen = 400\n",
        "kernel_size = 3\n",
        "DIM = 512\n",
        "batch_size = 512\n",
        "\n",
        "print('num_words = 40000, maxlen = 400')\n",
        "\n",
        "# fact数据集\n",
        "fact = np.load('/content/datadeal/fact_pad_seq/big_fact_pad_seq_40000_400.npy')\n",
        "fact_train, fact_test = train_test_split(fact, test_size=0.05, random_state=1)\n",
        "del fact\n",
        "\n",
        "# 标签数据集\n",
        "labels = np.load('/content/datadeal/labels/_accusation.npy')\n",
        "labels_train, labels_test = train_test_split(labels, test_size=0.05, random_state=1)\n",
        "del labels\n",
        "\n",
        "# 数据增强\n",
        "maxcount = 60000\n",
        "num = 10\n",
        "# index_add_accusation = np.load('./data_deal/index_add_accusation_%d_%d.npy' % (maxcount, num))\n",
        "# fact_train = np.concatenate([fact_train, fact_train[index_add_accusation]], axis=0)\n",
        "# labels_train = np.concatenate([labels_train, labels_train[index_add_accusation]], axis=0)\n",
        "\n",
        "data_input = Input(shape=[maxlen])\n",
        "word_vec = Embedding(input_dim=num_words + 1,\n",
        "                     input_length=maxlen,\n",
        "                     output_dim=DIM,\n",
        "                     mask_zero=0,\n",
        "                     name='Embedding')(data_input)\n",
        "block1 = block(x=word_vec, kernel_size=kernel_size)\n",
        "block2 = block(x=block1, kernel_size=kernel_size)\n",
        "x = GlobalMaxPool1D()(block2)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(1000, activation=\"relu\")(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(labels_train.shape[1], activation=\"sigmoid\")(x)\n",
        "model = Model(inputs=data_input, outputs=x)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "n_start = 6\n",
        "n_end = 13\n",
        "score_list1 = []\n",
        "score_list2 = []\n",
        "for i in range(n_start, n_end):\n",
        "    model.fit(x=fact_train, y=labels_train, batch_size=batch_size, epochs=1, verbose=2)\n",
        "\n",
        "    model.save('/content/model/RES_epochs_%d.h5' % (num_words, maxlen, i))\n",
        "\n",
        "    y = model.predict(fact_test[:])\n",
        "    y1 = predict2top(y)\n",
        "    y2 = predict2half(y)\n",
        "    y3 = predict2both(y)\n",
        "\n",
        "    print('%s accu:' % i)\n",
        "    # 只取最高置信度的准确率\n",
        "    s1 = [(labels_test[i] == y1[i]).min() for i in range(len(y1))]\n",
        "    print(sum(s1) / len(s1))\n",
        "    # 只取置信度大于0.5的准确率\n",
        "    s2 = [(labels_test[i] == y2[i]).min() for i in range(len(y1))]\n",
        "    print(sum(s2) / len(s2))\n",
        "    # 结合前两个\n",
        "    s3 = [(labels_test[i] == y3[i]).min() for i in range(len(y1))]\n",
        "    print(sum(s3) / len(s3))\n",
        "\n",
        "    print('%s f1:' % i)\n",
        "    # 只取最高置信度的准确率\n",
        "    s4 = f1_avg(y_pred=y1, y_true=labels_test)\n",
        "    print(s4)\n",
        "    # 只取置信度大于0.5的准确率\n",
        "    s5 = f1_avg(y_pred=y2, y_true=labels_test)\n",
        "    print(s5)\n",
        "    # 结合前两个\n",
        "    s6 = f1_avg(y_pred=y3, y_true=labels_test)\n",
        "    print(s6)\n",
        "\n",
        "    score_list1.append([i,\n",
        "                        sum(s1) / len(s1),\n",
        "                        sum(s2) / len(s2),\n",
        "                        sum(s3) / len(s3)])\n",
        "    score_list2.append([i, s4, s5, s6])\n",
        "\n",
        "print(pd.DataFrame(score_list1))\n",
        "print(pd.DataFrame(score_list2))\n",
        "print('end', time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
        "print('#####################\\n')\n",
        "# nohup python model_CNN_accusation.py 2>&1 &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8t109OIt9Fv",
        "outputId": "1bec3d35-ff64-407d-fd54-460b22d19521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start 2024-04-21 08:16:04\n",
            "accusation\n",
            "num_words = 40000, maxlen = 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Se9qu8XZSCf"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "localpath = os.path.dirname(__file__)\n",
        "\n",
        "\n",
        "class Predictor:\n",
        "  #改一下路径\n",
        "    def __init__(self, num_words=80000, maxlen=400,\n",
        "                 accusation_path=localpath + '/model/accusation/DIM_256_CNN_BN_gpool_epochs_19.h5',\n",
        "                 relevant_articles_path=localpath + '/model/relevant_articles/DIM_512_RES_BN_gpool_bs_256_epochs_18.h5',\n",
        "                 imprisonments_path=localpath + '/model/imprisonments/DIM_512_CNN_gpool_BN_epochs_10.h5',\n",
        "                 tokenizer_path=localpath + '/model/tokenizer_fact_80000.pkl'):\n",
        "        self.num_words = num_words\n",
        "        self.maxlen = maxlen\n",
        "        self.accusation_path = accusation_path\n",
        "        self.relevant_articles_path = relevant_articles_path\n",
        "        self.batch_size = 512\n",
        "        self.content_transform = data_transform()\n",
        "        self.tokenizer_path = tokenizer_path\n",
        "        self.model1 = load_model(accusation_path)\n",
        "        self.model2 = load_model(relevant_articles_path)\n",
        "        self.model3 = load_model(imprisonments_path)\n",
        "\n",
        "    def predict(self, content):\n",
        "        num_words = self.num_words\n",
        "        maxlen = self.maxlen\n",
        "        content_transform = self.content_transform\n",
        "        tokenizer_path = self.tokenizer_path\n",
        "        # 分词\n",
        "        content_cut = content_transform.cut_texts(texts=content, word_len=2)\n",
        "        with open(tokenizer_path, mode='rb') as f:\n",
        "            tokenizer_fact = pickle.load(f)\n",
        "        content_transform.text2seq(texts_cut=content_cut, tokenizer_fact=tokenizer_fact,\n",
        "                                   num_words=num_words, maxlen=maxlen)\n",
        "        content_fact_pad_seq = np.array(content_transform.fact_pad_seq)\n",
        "\n",
        "        model1 = self.model1\n",
        "        accusation = model1.predict(content_fact_pad_seq)\n",
        "        model2 = self.model2\n",
        "        relevant_articles = model2.predict(content_fact_pad_seq)\n",
        "        model3 = self.model3\n",
        "        imprisonments = model3.predict(content_fact_pad_seq)\n",
        "\n",
        "        def transform(x):\n",
        "            n = len(x)\n",
        "            x_return = np.arange(1, n + 1)[x > 0.5].tolist()\n",
        "            if len(x_return) == 0:\n",
        "                x_return = np.arange(1, n + 1)[x == x.max()].tolist()\n",
        "            return x_return\n",
        "\n",
        "        result = []\n",
        "        for i in range(0, len(content)):\n",
        "            if imprisonments[i][0] > 400:\n",
        "                imprisonment = -2\n",
        "            elif imprisonments[i][0] > 300:\n",
        "                imprisonment = -1\n",
        "            else:\n",
        "                imprisonment = int(np.round(imprisonments[i][0], 0))\n",
        "\n",
        "            result.append({\n",
        "                \"accusation\": transform(accusation[i]),\n",
        "                \"articles\": transform(relevant_articles[i]),\n",
        "                \"imprisonment\": imprisonment\n",
        "            })\n",
        "        return result\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    content = ['我爱北京天安门', '收款方哪家口碑北京开始，数据库备份围绕健康上网电费']\n",
        "    predictor = Predictor()\n",
        "    m = predictor.predict(content)\n",
        "    print(m)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1jXNmGXmFURNDaUfYSW_blauqjLbQj9u7",
      "authorship_tag": "ABX9TyNDQD1e9XzZufYUGOiL5KwC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}